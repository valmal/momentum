{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###################################\n",
    "#       Loading Packages          #\n",
    "###################################\n",
    "import cufflinks as cf\n",
    "cf.go_online()\n",
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import glob\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#        Imporing Data            #\n",
    "###################################\n",
    "\n",
    "#define path\n",
    "path=r\"/Users/valeriyamalenko/Desktop/trading/trading/momentum/data/mecklai-finance/\"\n",
    "\n",
    "#list paths to all usd-base files\n",
    "files=glob.glob(path +\"/usd-*.csv\")\n",
    " \n",
    "#open the first file\n",
    "tb=pd.read_csv(files[0])\n",
    "\n",
    "#append all other files in the directory to the first file\n",
    "for i in files[1:]:\n",
    "    tb1=pd.read_csv(i, header=0)\n",
    "    tb=tb.append(tb1)\n",
    "    \n",
    "tb=tb[['Currency', 'Date', 'SpotDate', 'SpotRate', 'col1']]\n",
    "tb=tb.drop_duplicates()\n",
    "tb=tb.reset_index()\n",
    "tb.drop('index', inplace=True, axis='columns')\n",
    "tb = tb.rename(columns = {'col1': 'frd1m'})\n",
    "len(tb.Currency.unique().tolist())\n",
    "\n",
    "#list paths to usd-term files\n",
    "files=glob.glob(path +\"/*-usd.csv\")\n",
    "\n",
    "#open the first file\n",
    "tb2=pd.read_csv(files[0])\n",
    "\n",
    "#append all other files in the directory to the first file\n",
    "for i in files[1:]:\n",
    "    tb3=pd.read_csv(i, header=0)\n",
    "    tb2=tb2.append(tb3)\n",
    "    \n",
    "tb2=tb2[['Currency', 'Date', 'SpotDate', 'SpotRate', 'col1']]\n",
    "tb2=tb2.drop_duplicates()\n",
    "tb2=tb2.reset_index()\n",
    "tb2.drop('index', inplace=True, axis='columns')\n",
    "tb2=tb2.rename(columns = {'col1': 'frd1m'})\n",
    "\n",
    "#transorming pairs to be quoted against the dollar (e.g. USD/GBP)\n",
    "tb2.SpotRate=1/tb2.SpotRate\n",
    "tb2.frd1m=1/tb2.frd1m\n",
    "\n",
    "a=tb2.Currency.str.split(\"/\")\n",
    "tb2[\"Currency\"]=a.str[1]+\"/\"+a.str[0]\n",
    "len(tb2.Currency.unique().tolist())\n",
    "\n",
    "#adding usd-term and usd-base dfs\n",
    "frames=[tb2,tb]\n",
    "df=pd.concat(frames)\n",
    "df.rename(columns = {'Date': 'date', 'Currency': 'ccy', 'SpotRate': 'spot'}, inplace = True)\n",
    "\n",
    "\n",
    "##################################\n",
    "#  Cleaning and Inspecting Data  #\n",
    "##################################\n",
    "\n",
    "#convert date to datetime\n",
    "df.date=pd.to_datetime(df['date'])\n",
    "\n",
    "#set date to be the index\n",
    "df=df.set_index('date')\n",
    "\n",
    "#exclude 2019 data\n",
    "dt = datetime.datetime(2019, 1, 1)\n",
    "df = df.iloc[df.index < dt]\n",
    "\n",
    "df.reset_index().groupby('date').count().sort_values(by='ccy', ascending = False)\n",
    "\n",
    "#the dataset clearly has a problem: duplicates\n",
    "\n",
    "df= df.reset_index()  \n",
    "\n",
    "#drop duplicates\n",
    "ccy_l=df['ccy'].unique().tolist()\n",
    "i=ccy_l[0]\n",
    "df_1=df[df['ccy']==i]\n",
    "df_1=df_1.drop_duplicates(['date'])\n",
    "\n",
    "for i in ccy_l[1:]:\n",
    "    df_2=df[df['ccy']==i]\n",
    "    df_2=df_2.drop_duplicates(['date'])\n",
    "    df_1=df_1.append(df_2)\n",
    "df=df_1\n",
    "\n",
    "df =df.set_index('date')\n",
    "\n",
    "#select last day of the month for each month\n",
    "df=df.loc[df.reset_index().groupby(df.index.to_period('M'))['date'].max()]\n",
    "len(df) # good!  12*24*8 = 2304\n",
    "\n",
    "df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "#      Transofrming Data         #\n",
    "##################################\n",
    "\n",
    "#below is included to suppress the warning\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "#1#generate new cols for logs of spot and fwd rates\n",
    "df['logSpotRate']=np.log(df['spot'])\n",
    "df['logfwd1m']=np.log(df['frd1m'])\n",
    "\n",
    "#2#shift forward rates \n",
    "ret_df = df[['ccy', 'logSpotRate', 'logfwd1m', 'date']]\n",
    "ret_df = ret_df.pivot_table(index = 'date', columns = ['ccy'])\n",
    "\n",
    "for i in df.ccy.unique().tolist():\n",
    "    ret_df[('logfwd1m', i)] =  ret_df[('logfwd1m', i)].shift(1)\n",
    "ret_df = ret_df[1:]\n",
    "\n",
    "for i in df.ccy.unique().tolist():\n",
    "    ret_df[('ret', i)] = ret_df[('logfwd1m', i)] - ret_df[('logSpotRate', i)] \n",
    "\n",
    "ret_df = pd.DataFrame(ret_df['ret'].unstack())\n",
    "ret_df.reset_index(inplace=True)\n",
    "ret_df.set_index('date', inplace = True)\n",
    "ret_df.reset_index(inplace=True)\n",
    "ret_df.rename(columns = {0: 'spot'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Create Momentum Portfolio      #\n",
    "##################################\n",
    "\n",
    "N = 3 # number of portfolios\n",
    "J = [1, 3, 6, 9, 12] # formation Period Length: J can be between 3 to 12 months\n",
    "K =[1, 3, 6, 9, 12] # folding Period Length: K can be between 3 to 12 months\n",
    "\n",
    "ret_df = ret_df[['ccy','date','spot']].sort_values(['ccy','date']).set_index('date')\n",
    "\n",
    "#falculate rolling cumulative return\n",
    "#by summing log(1+ret) over the formation period\n",
    "ret_df['logret']=np.log(1+ret_df['spot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frames = {}\n",
    "df_cumulative = {}\n",
    "for j,k in itertools.product(J, K):\n",
    "    ret_df1 = ret_df.copy()\n",
    "    umd = ret_df1.groupby(['ccy'])['logret'].rolling(j, min_periods=j).sum()\n",
    "    umd = umd.reset_index()\n",
    "    umd['cumret']=np.exp(umd['logret'])-1\n",
    "    \n",
    "    \n",
    "     ##################################\n",
    "     #   Formation of 3  Portfolios   #\n",
    "     ##################################\n",
    "    \n",
    "    #for each date: assign ranking 1-3 based on cumret 1=lowest 3=highest cumret\n",
    "    umd=umd.dropna(axis=0, subset=['cumret'])\n",
    "    umd['momr']=umd.groupby('date')['cumret'].transform(lambda x: pd.qcut(x, N, labels=False))\n",
    "    umd.momr=umd.momr.astype(int)\n",
    "    umd['momr'] = umd['momr']+1\n",
    "    umd['hdate1']=umd['date']+pd.tseries.offsets.MonthBegin(1)\n",
    "    umd['hdate2']=umd['date']+pd.tseries.offsets.MonthEnd(k)\n",
    "    umd=umd.rename(columns={'date':'form_date'})\n",
    "    umd = umd[['ccy','form_date','momr','hdate1','hdate2']]\n",
    "    \n",
    "    #join rank and return data together\n",
    "    #note: this step consumes a lot of memory so takes a while\n",
    "    ret_df1.reset_index(inplace = True)\n",
    "    _tmp_ret = ret_df1[['ccy','date','spot']]\n",
    "    port = pd.merge(_tmp_ret, umd, on=['ccy'], how='inner')\n",
    "    port = port[(port['hdate1']<=port['date']) & (port['date']<=port['hdate2'])]\n",
    "    \n",
    "    umd2 = port.sort_values(by=['date','momr','form_date','ccy']).drop_duplicates()\n",
    "    umd3 = umd2.groupby(['date','momr','form_date'])['spot'].mean().reset_index()\n",
    "    \n",
    "    #skip first two years of the sample \n",
    "    start_yr = umd3['date'].dt.year.min()+2\n",
    "    umd3 = umd3[umd3['date'].dt.year>=start_yr]\n",
    "    umd3 = umd3.sort_values(by=['date','momr'])\n",
    "    \n",
    "    #create one return series per MOM group every month\n",
    "    ewret = umd3.groupby(['date','momr'])['spot'].mean().reset_index()\n",
    "    ewstd = umd3.groupby(['date','momr'])['spot'].std().reset_index()\n",
    "    ewret = ewret.rename(columns={'spot':'ewret'})\n",
    "    ewstd = ewstd.rename(columns={'spot':'ewretstd'})\n",
    "    ewretdat = pd.merge(ewret, ewstd, on=['date','momr'], how='inner')\n",
    "    ewretdat = ewretdat.sort_values(by=['momr', 'date'])\n",
    "    ewretdat\n",
    "    # portfolio summary\n",
    "    ewretdat.groupby(['momr'])['ewret'].describe()[['count', 'mean', 'std']]\n",
    "    \n",
    "    \n",
    "    ##################################\n",
    "    #  Long-Short Portfolio Returns  #\n",
    "    ##################################\n",
    "    \n",
    "    #transpose portfolio layout to have columns as portfolio returns\n",
    "    ewretdat2 = ewretdat.pivot(index='date', columns='momr', values='ewret')\n",
    "    \n",
    "    #add prefix port in front of each column\n",
    "    ewretdat2 = ewretdat2.add_prefix('port')\n",
    "    ewretdat2 = ewretdat2.rename(columns={'port1':'losers', 'port'+str(N):'winners'})\n",
    "    ewretdat2['long_short'] = ewretdat2['winners'] - ewretdat2['losers']\n",
    "    \n",
    "    #annualised return\n",
    "    ewretdat3=ewretdat2[['long_short']].add(1).prod() ** (12 / len(ewretdat2['long_short'])) - 1\n",
    "    ewretdat3 = ewretdat3.to_frame().rename(columns={0:'Excess Return'}).reset_index()\n",
    "    \n",
    "    ewretdat3['f'] = j\n",
    "    ewretdat3['h'] = k\n",
    "    ewretdat3[['T-Statistic','p-value']] = pd.Series(stats.ttest_1samp(ewretdat2['long_short'],0.0)).to_frame().T\n",
    "    ewretdat3['Sharpe Ratio'] = (ewretdat2['long_short'].mean()*12)/(ewretdat2['long_short'].std()*np.sqrt(12))\n",
    "    df_frames[(j,k)] = ewretdat3\n",
    "   \n",
    "    #compute Long-Short Portfolio Cumulative Returns\n",
    "    ewretdat4 = ewretdat2\n",
    "    ewretdat4['1+losers']=1+ewretdat2['losers']\n",
    "    ewretdat4['1+winners']=1+ewretdat2['winners']\n",
    "    ewretdat4['1+ls'] = 1+ewretdat2['long_short']\n",
    "    \n",
    "    ewretdat4['cumret_winners']=ewretdat4['1+winners'].cumprod()-1\n",
    "    ewretdat4['cumret_losers']=ewretdat4['1+losers'].cumprod()-1\n",
    "    ewretdat4['cumret_long_short']=ewretdat4['1+ls'].cumprod()-1\n",
    "    ewretdat4['f'] = j\n",
    "    ewretdat4['h'] = k\n",
    "    df_cumulative[(j,k)]=ewretdat4\n",
    "\n",
    "df = pd.concat(df_frames.values(), ignore_index=True)\n",
    "df = df.pivot_table(values =['T-Statistic','Excess Return', 'Sharpe Ratio'], \n",
    "                    columns ='h', index = 'f')\n",
    "\n",
    "df1 = df.style.format({\n",
    "    (\"Excess Return\", 1): lambda x: \"{:.2%}\".format(x),\n",
    "    (\"Excess Return\", 3): lambda x: \"{:.2%}\".format(x),\n",
    "    (\"Excess Return\", 6): lambda x: \"{:.2%}\".format(x),\n",
    "    (\"Excess Return\", 9): lambda x: \"{:.2%}\".format(x),\n",
    "    (\"Excess Return\", 12): lambda x: \"{:.2%}\".format(x),\n",
    "    \n",
    "    (\"Sharpe Ratio\", 1): lambda x: \"{:.2}\".format(x),\n",
    "    (\"Sharpe Ratio\", 3): lambda x: \"{:.2}\".format(x),\n",
    "    (\"Sharpe Ratio\", 6): lambda x: \"{:.2}\".format(x),\n",
    "    (\"Sharpe Ratio\", 9): lambda x: \"{:.2}\".format(x),\n",
    "    (\"Sharpe Ratio\", 12): lambda x: \"{:.2}\".format(x),\n",
    "    \n",
    "    (\"T-Statistic\", 1): lambda x: \"{:.2}\".format(x),\n",
    "    (\"T-Statistic\", 3): lambda x: \"{:.2}\".format(x),\n",
    "    (\"T-Statistic\", 6): lambda x: \"{:.2}\".format(x),\n",
    "    (\"T-Statistic\", 9): lambda x: \"{:.2}\".format(x),\n",
    "    (\"T-Statistic\", 12): lambda x: \"{:.2}\".format(x)}).set_caption('Table 1. Momentum Returns and Sharpe Ratios')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sns.light_palette(\"green\", as_cmap=True);\n",
    "df1.background_gradient(cmap=cm, low =0, high=0.99);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(df_cumulative.values(), ignore_index=False)\n",
    "df['f/h'] = list(zip(df.f, df.h))\n",
    "df.drop(['h', 'f'], axis=1, inplace=True)\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#           Visualising the data              #\n",
    "##############################################\n",
    "\n",
    "#ret_df.pivot(columns = 'ccy', index = 'date', values = 'spot')\\\n",
    "#.plot(kind='line', subplots=True, grid=True, title=\"Currency Monthly Returns\",\n",
    "#         sharex=True, sharey=True, legend=True, figsize=(16, 16), layout=(6,4),\n",
    "#       style=['g', 'b', 'b', 'b', 'r', 'r', 'b', 'b', 'g','r', 'r', 'b','r', 'b', 'b','r', 'g', 'r','r', 'r', 'r']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPanda = [];\n",
    "for i in df['f/h'].unique().tolist():\n",
    "    new =df[df['f/h']==i]\n",
    "    trace = go.Scatter(\n",
    "    x = list(new['date']),\n",
    "    y = list(new['cumret_long_short']*100), \n",
    "    name=str(i))\n",
    "    \n",
    "    dataPanda.append(trace) \n",
    "layout = go.Layout(\n",
    "    title='Figure 1. Cumulative Excess returns of momentum strategies',\n",
    "    yaxis=dict(\n",
    "        title='Cumulative Excess Returns (in %)',\n",
    "        titlefont=dict(\n",
    "            size=18,\n",
    "            color='#7f7f7f')),\n",
    "    width=1000,\n",
    "    height=700,\n",
    " annotations=[\n",
    "        dict(\n",
    "            x=1.09,\n",
    "            y=1.03,\n",
    "            align=\"center\",\n",
    "            valign=\"top\",\n",
    "            text='f, h',\n",
    "            showarrow=False,\n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\",\n",
    "            xanchor=\"center\",\n",
    "            yanchor=\"top\")])\n",
    "     \n",
    "fig = dict(data=dataPanda,layout=layout )\n",
    "config={'showLink': False} \n",
    "#iplot(fig, config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
